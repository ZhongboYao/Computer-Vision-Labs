{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1742157833153,
     "user": {
      "displayName": "Jonathan Pine",
      "userId": "12663940060477254531"
     },
     "user_tz": -60
    },
    "id": "yekRmvRK5aD9"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1742157891805,
     "user": {
      "displayName": "Jonathan Pine",
      "userId": "12663940060477254531"
     },
     "user_tz": -60
    },
    "id": "6Xx2Z_WJ7NZ6"
   },
   "outputs": [],
   "source": [
    "def get_frame_time(cap) -> int:\n",
    "  return int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "def between(cap, lower: int, upper: int) -> bool:\n",
    "    return lower <= get_frame_time(cap) < upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_scale(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(frame, kernel_size):\n",
    "    blurred = cv2.GaussianBlur(frame, (kernel_size, kernel_size), 15)\n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilateral_blur(frame, d):\n",
    "    filtered = cv2.bilateralFilter(frame, d, 50, 50)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr_mask(frame, lower_bound, upper_bound):\n",
    "    mask = cv2.inRange(frame, lower_bound, upper_bound)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphology_operation(mask, method, kernel):\n",
    "    mask_enhanced = cv2.morphologyEx(mask, method, kernel)\n",
    "    return mask_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_filter(frame, ksize, scale=1, delta=0):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    kx, ky = cv2.getDerivKernels(dx=1, dy=0, ksize=ksize, normalize=False, ktype=cv2.CV_64F)\n",
    "    sobel_kernel_x = np.outer(kx, ky)\n",
    "    abs_sum_x = np.sum(np.abs(sobel_kernel_x))\n",
    "    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize, scale=1/abs_sum_x, delta=delta) # Normalize the brightness.\n",
    "\n",
    "    kx, ky = cv2.getDerivKernels(dx=0, dy=1, ksize=ksize, normalize=False, ktype=cv2.CV_64F)\n",
    "    sobel_kernel_y = np.outer(kx, ky)\n",
    "    abs_sum_y = np.sum(np.abs(sobel_kernel_y))\n",
    "    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize, scale=1/abs_sum_y, delta=delta)\n",
    "\n",
    "    abs_sobel_x = cv2.convertScaleAbs(sobel_x)\n",
    "    abs_sobel_y = cv2.convertScaleAbs(sobel_y)\n",
    "\n",
    "    sobel_combined = cv2.addWeighted(abs_sobel_x, 0.5, abs_sobel_y, 0.5, 0)\n",
    "    rescaled_frame = cv2.normalize(sobel_combined, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    return rescaled_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_trans(frame, blur_size, dp, param1, param2):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_blur = cv2.medianBlur(frame_gray, blur_size) # Better than Gaussian, faster than Bilateral.\n",
    "    circles = cv2.HoughCircles(frame_blur, cv2.HOUGH_GRADIENT, dp=dp, minDist=100,\n",
    "                            param1=param1, param2=param2, minRadius=0, maxRadius=0)\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for c in circles[0, :]:\n",
    "            cv2.circle(frame, (c[0], c[1]), c[2], (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (c[0], c[1]), 2, (0, 0, 255), 3)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_matching(template_path, frame, alpha, probability_mode=0):\n",
    "    template = cv2.imread(template_path)\n",
    "    tH, tW = template.shape[:2]\n",
    "\n",
    "    edges_video = sobel_filter(frame, 3)\n",
    "    edges_template = sobel_filter(template, 5)\n",
    "\n",
    "    result_color = cv2.matchTemplate(frame, template, cv2.TM_SQDIFF_NORMED)\n",
    "    result_edges = cv2.matchTemplate(edges_video, edges_template, cv2.TM_SQDIFF_NORMED)\n",
    "    \n",
    "    final_result = alpha * result_color + (1 - alpha) * result_edges # The match result considers weighted color and edges matching.\n",
    "    _, _, minLoc, _ = cv2.minMaxLoc(final_result)\n",
    "    top_left = minLoc\n",
    "    bottom_right = (top_left[0] + tW, top_left[1] + tH) # Rectangular size is set to the template size\n",
    "    cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 3)\n",
    "\n",
    "    output_frame = frame\n",
    "    output_frame_probability = None\n",
    "    \n",
    "    if probability_mode:\n",
    "        likelihood = 1 - final_result  # Reverse of match result = probability\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize as the likelihood has a smaller dimension due to correlation operation.\n",
    "        likelihood_upsampled = cv2.resize(likelihood, (gray.shape[1], gray.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        likelihood_scaled = np.uint8(255 * likelihood_upsampled) # Change to normal values.\n",
    "        output_frame_probability = cv2.cvtColor(likelihood_scaled, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return output_frame, output_frame_probability\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(mask, threshold):\n",
    "    # Retrieve outmost contour.\n",
    "    initial_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = []\n",
    "    for contour in initial_contours:\n",
    "        # Filter out small contours as they are noises.\n",
    "        if cv2.contourArea(contour) > threshold:\n",
    "            contours.append(contour)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision_pos_cal(prev_detections, detections):\n",
    "    \"\"\"\n",
    "    This method is based on the idea that when there's a collision, there are rectangulars merged.\n",
    "    So the rectangular that has an abnormal shape indicates it has the collision.\n",
    "    This method is only safe when other objects' shapes do not change vastly.\n",
    "    The collision place is assumed to be the center of the merged rectangular.\n",
    "    \"\"\"\n",
    "    avg_width = np.mean([br[0] - tl[0] for tl, br in prev_detections])\n",
    "    avg_height = np.mean([br[1] - tl[1] for tl, br in prev_detections])\n",
    "    for tl, br in detections:\n",
    "        curr_width = br[0] - tl[0]\n",
    "        curr_height = br[1] - tl[1]\n",
    "        # The merged rectangular is the one which gets bigger.\n",
    "        if curr_width > 1.5 * avg_width:  \n",
    "            collision_center = ((tl[0] + br[0]) // 2, (tl[1] + br[1]) // 2)\n",
    "            break\n",
    "        elif curr_height > 1.5 * avg_height:\n",
    "            collision_center = ((tl[0] + br[0]) // 2, (tl[1] + br[1]) // 2)\n",
    "            break\n",
    "    return collision_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision_effect(collision_center, frame):\n",
    "    explosion_img = cv2.imread(\"explosion.png\", cv2.IMREAD_UNCHANGED)\n",
    "    eh, ew = explosion_img.shape[:2]\n",
    "    cx, cy = collision_center\n",
    "    \n",
    "    # Top-left position of the effect\n",
    "    top_left_x = cx - ew // 2\n",
    "    top_left_y = cy - eh // 2\n",
    "\n",
    "    # Calculate bottom-right corner\n",
    "    bottom_right_x = top_left_x + ew\n",
    "    bottom_right_y = top_left_y + eh\n",
    "\n",
    "    # Effect region\n",
    "    roi_width = bottom_right_x - top_left_x\n",
    "    roi_height = bottom_right_y - top_left_y\n",
    "\n",
    "    frame_roi = frame[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "    explosion_roi = explosion_img[:roi_height, :roi_width]\n",
    "    _, _, _, explosion_a = cv2.split(explosion_roi)\n",
    "\n",
    "    # Normalize the alpha mask to [0.0, 1.0]\n",
    "    alpha = explosion_a.astype(float) / 255.0\n",
    "    inv_alpha = 1.0 - alpha\n",
    "\n",
    "    # Perform alpha blending\n",
    "    for c in range(3):\n",
    "        frame_roi[:, :, c] = (alpha * explosion_roi[:, :, c] + inv_alpha * frame_roi[:, :, c]).astype(frame_roi.dtype)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, max_width):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    lines = []\n",
    "    current_line = words[0]\n",
    "\n",
    "    # If the text length exceeds the maximum length, automatically start a new line for it.\n",
    "    for word in words[1:]:\n",
    "        test_line = current_line + \" \" + word\n",
    "        (test_width, _), _ = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        if test_width > max_width:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "        else:\n",
    "            current_line = test_line\n",
    "\n",
    "    lines.append(current_line)  \n",
    "    return lines\n",
    "\n",
    "def add_subtitles_to_frame(subtitles, frame, current_time):\n",
    "    active_subtitles = [\n",
    "        s for s in subtitles if s[\"start\"] <= current_time <= s[\"end\"]\n",
    "    ]\n",
    "\n",
    "    lines_to_draw = []\n",
    "    for subtitle in active_subtitles:\n",
    "        wrapped_lines = wrap_text(subtitle[\"text\"], 1500)\n",
    "        lines_to_draw.extend(wrapped_lines)\n",
    "\n",
    "    # text_sizes[i] = ((width, height), baseline)\n",
    "    text_sizes = [\n",
    "        cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        for line in lines_to_draw\n",
    "    ]\n",
    "    \n",
    "    max_line_width = max(sz[0][0] for sz in text_sizes) if text_sizes else 0\n",
    "    max_line_height = max(sz[0][1] for sz in text_sizes) if text_sizes else 0\n",
    "\n",
    "    # Padding for rectangulars, which are behind captions to make captions more clear.\n",
    "    rect_width = max_line_width + 40     \n",
    "    rect_height = max_line_height + 20\n",
    "\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    total_lines = len(lines_to_draw)\n",
    "    total_block_height = rect_height * total_lines\n",
    "\n",
    "    # 10 pixels are kept between the rectangular bottom and the frame bottom.\n",
    "    y_start = frame_height - total_block_height - 10\n",
    "\n",
    "    for i, line in enumerate(lines_to_draw):\n",
    "        # Calculate the rectangular position and plot it.\n",
    "        x_rect = (frame_width - rect_width) // 2 # So that the rectangular is put in the middle.\n",
    "        y_rect = y_start + i * rect_height\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (x_rect, y_rect),\n",
    "            (x_rect + rect_width, y_rect + rect_height),\n",
    "            (0, 0, 0), \n",
    "            thickness=-1\n",
    "        )\n",
    "\n",
    "        # Get this line's position info.\n",
    "        (line_width, line_height), baseline = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        x_text = x_rect + (rect_width - line_width) // 2 \n",
    "        y_text = int(y_rect + (rect_height + line_height) / 2 - baseline / 2) \n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            line,\n",
    "            (x_text, y_text),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.0,\n",
    "            (255, 255, 255),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742157892986,
     "user": {
      "displayName": "Jonathan Pine",
      "userId": "12663940060477254531"
     },
     "user_tz": -60
    },
    "id": "yCnuMcu16DP-"
   },
   "outputs": [],
   "source": [
    "def main(input_video_file: str, output_video_file: str) -> None:\n",
    "    cap = cv2.VideoCapture(input_video_file)\n",
    "\n",
    "    if cap is None or not cap.isOpened():\n",
    "      raise RuntimeError('The file was not found or is not a proper video.')\n",
    "\n",
    "    fps = int(round(cap.get(5)))\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')       \n",
    "    out = cv2.VideoWriter(output_video_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    prev_num_objs = 0\n",
    "    collision_frames = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        subtitles = []\n",
    "        if ret:\n",
    "            if cv2.waitKey(28) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            time = get_frame_time(cap)\n",
    "\n",
    "            # Switch between color and gray for the first 4 seconds.\n",
    "            if between(cap, 0, 500) or between(cap, 1000, 1500) or between(cap, 2000, 2500) or between(cap, 3000, 3500):\n",
    "                frame = gray_scale(frame)\n",
    "            subtitles = [\n",
    "                {\n",
    "                    \"start\": 0,\n",
    "                    \"end\": 5000,\n",
    "                    \"text\": \"Switching the color between BGR and gray is done using cv2.cvtColor().\"\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Gaussian blur.\n",
    "            if between(cap, 4000, 8000):\n",
    "                kernel_size = int(1 + (time - 4000) * (50 - 1) / (8000 - 4000)) # Change kernel size from 1 to 50.\n",
    "                if kernel_size % 2 == 0:\n",
    "                    kernel_size += 1\n",
    "                frame = gaussian_blur(frame, kernel_size)\n",
    "            \n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 4000,\n",
    "                        \"end\": 8000,\n",
    "                        \"text\": f'''Gaussian blur, kernel_size = {kernel_size}'''\n",
    "                    },\n",
    "                    {\n",
    "                        \"start\": 4000,\n",
    "                        \"end\": 8000,\n",
    "                        \"text\": f'''Gaussian filter calculates weighted averages by only considering spatial proximity, \n",
    "                                    but Bilateral filter considers both spatial and intensity proximity. n\n",
    "                                    If two pixels\\' intensities have a large difference, then the corresponding weight will be small. \n",
    "                                    Therefore, the two pixels do not affect each other very much, preserving the edges in the image.'''\n",
    "                    }\n",
    "                ]\n",
    "            \n",
    "            # Bilateral blur.\n",
    "            if between(cap, 8000, 12000):\n",
    "                kernel_size = int(1 + (time - 8000) * (50 - 1) / (12000 - 8000)) # Change kernel size from 1 to 50.\n",
    "                if kernel_size % 2 == 0:\n",
    "                    kernel_size += 1\n",
    "                frame = bilateral_blur(frame, kernel_size)\n",
    "            \n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 8000,\n",
    "                        \"end\": 12000,\n",
    "                        \"text\": f'''Bilateral blur, kernel_size = {kernel_size}'''\n",
    "                    },\n",
    "                    {\n",
    "                        \"start\": 8000,\n",
    "                        \"end\": 12000,\n",
    "                        \"text\": f'''Gaussian filter calculates weighted averages by only considering spatial proximity, \n",
    "                                    but Bilateral filter considers both spatial and intensity proximity.\n",
    "                                    If two pixels\\' intensities have a large difference, then the corresponding weight will be small. \n",
    "                                    Therefore, the two pixels do not affect each other very much, preserving the edges in the image.\n",
    "                                    In terms of the effect, Gaussian blur looks like myopia, Bilateral polishes surfaces.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            # Grab objects using BGR channels.\n",
    "            if between(cap, 12000, 20000):\n",
    "                lower_bound = np.array([0, 0, 99])\n",
    "                upper_bound = np.array([42, 120, 255])\n",
    "                mask = bgr_mask(frame, lower_bound, upper_bound)\n",
    "                frame = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                kernel = np.ones((10, 10), np.uint8)\n",
    "                mask_enhanced = morphology_operation(mask, cv2.MORPH_OPEN, kernel)\n",
    "                mask_enhanced = morphology_operation(mask_enhanced, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "                added_pixels = cv2.subtract(mask_enhanced, mask)\n",
    "                deleted_pixels = cv2.subtract(mask, mask_enhanced)\n",
    "                frame[added_pixels == 255] = (0, 255, 0)\n",
    "                frame[deleted_pixels == 255] = (0, 0, 255)\n",
    "            \n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 12000,\n",
    "                        \"end\": 20000,\n",
    "                        \"text\": f'''Tangerine are moslty identified. Other white places are noises introduced by the light.\n",
    "                                    To remove the noises, OPENING is used. Red pixels are the pixels removed by erosion and green ones are \n",
    "                                    the pixels added by dilation.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            # Sobel edges filter.\n",
    "            if between(cap, 20000, 25000):\n",
    "                kernel_size = int(1 + (time - 20000) * (31 - 1) / (25000 - 20000))\n",
    "                if kernel_size % 2 == 0:\n",
    "                    kernel_size += 1\n",
    "\n",
    "                edges = sobel_filter(frame, ksize=kernel_size)\n",
    "                frame = cv2.applyColorMap(edges, cv2.COLORMAP_DEEPGREEN)\n",
    "            \n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 20000,\n",
    "                        \"end\": 25000,\n",
    "                        \"text\": f'''Sobel_kernel_size = {kernel_size}'''\n",
    "                    },\n",
    "                    {\n",
    "                        \"start\": 20000,\n",
    "                        \"end\": 25000,\n",
    "                        \"text\": f'''As the kernel size increases, edges get thicker. They actually get brighter as they calculate more area and the central \n",
    "                                    weight are larger, but the brightness is normalized before showing each frame.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            # Hough transform.\n",
    "            if between(cap, 25000, 30000):\n",
    "                blursize1 = 15\n",
    "                dp1 = 1.3\n",
    "                param11 = 50\n",
    "                param21 = 65\n",
    "                frame = hough_trans(frame, blur_size=blursize1, dp=dp1, param1=param11, param2 = param21)\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 25000,\n",
    "                        \"end\": 30000,\n",
    "                        \"text\": f'''blur_size = {blursize1}, dp = {dp1}, param1 = {param11}, param2 = {param21}'''\n",
    "                    }\n",
    "                ]\n",
    "            # Try a new set for Hough Transform to explore the effect of different parameters.\n",
    "            if between(cap, 30000, 35000):\n",
    "                blursize2 = 11\n",
    "                dp2 = 1.5\n",
    "                param12 = 40\n",
    "                param22 = 60\n",
    "                frame = hough_trans(frame, blur_size=blursize2, dp=dp2, param1=param12, param2 = param22)\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 30000,\n",
    "                        \"end\": 35000,\n",
    "                        \"text\": f'''blur_size = {blursize2}, dp = {dp2}, param1 = {param12}, param2 = {param22}'''\n",
    "                    },\n",
    "                    {\n",
    "                        \"start\": 30000,\n",
    "                        \"end\": 35000,\n",
    "                        \"text\": f'''For this new set, which has a decrased blur size, param1 and param2 and an increased\n",
    "                                    dp, more noise circles are shown, vice versa. The new set leaves more noise edges due to\n",
    "                                    small blur kernel size, and they are not filtered out due to the decreased param1.\n",
    "                                    Param2 is decreased which also means the voting threshold is easier to be met, leading to more\n",
    "                                    noise cirlces. And with an increased dp, though it is coarser, noises can affect more buckets, thus \n",
    "                                    there are more noise circles.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            # Template matching.\n",
    "            if between(cap, 35000, 37000):\n",
    "                frame, _ = template_matching('template.png', frame, 0.7, 0)\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 35000,\n",
    "                        \"end\": 37000,\n",
    "                        \"text\": f'''The template is a tangerine image.'''\n",
    "                    }\n",
    "                ]\n",
    "            if between(cap, 37000, 40000):\n",
    "                _, frame = template_matching('template.png', frame, 0.7, 1)\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 37000,\n",
    "                        \"end\": 40000,\n",
    "                        \"text\": f'''The template is a tangerine image.'''\n",
    "                    }\n",
    "                ]\n",
    "            \n",
    "            # Free-style part.\n",
    "            if between(cap, 41000, 43000):\n",
    "                lower_bound = np.array([0, 0, 99])\n",
    "                upper_bound = np.array([42, 120, 255])\n",
    "                mask = bgr_mask(frame, lower_bound, upper_bound)\n",
    "                kernel = np.ones((30, 30), np.uint8)\n",
    "                mask = morphology_operation(mask, cv2.MORPH_DILATE, kernel)\n",
    "                frame = cv2.inpaint(frame, mask, 3, cv2.INPAINT_TELEA)\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 41000,\n",
    "                        \"end\": 43000,\n",
    "                        \"text\": f'''Make tangerines disappear.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            if between(cap, 43000, 55000):\n",
    "                lower_bound = np.array([0, 0, 99])\n",
    "                upper_bound = np.array([42, 120, 255])\n",
    "                mask = bgr_mask(frame, lower_bound, upper_bound)\n",
    "                kernel = np.ones((10, 10), np.uint8)\n",
    "                mask = morphology_operation(mask, cv2.MORPH_OPEN, kernel)\n",
    "                contours = find_contours(mask, 1000)\n",
    "\n",
    "                detections = []\n",
    "                for cnt in contours:\n",
    "                        x, y, w, h = cv2.boundingRect(cnt) # Get the object's shape.\n",
    "                        detections.append(((x, y), (x + w, y + h)))\n",
    "                \n",
    "                # Mark objects using rectangulars.\n",
    "                for i, (tl, br) in enumerate(detections):\n",
    "                    cv2.rectangle(frame, tl, br, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"ID: {i}\", tl, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                \n",
    "                # Detect collision by checking whether there is reduction in rectangulars num.\n",
    "                curr_num_objs = len(detections)\n",
    "                if curr_num_objs < prev_num_objs:\n",
    "                    if detections: # Make sure the reduction in rectangulars num is due to collision not the object left the scene.\n",
    "                        collision_center = collision_pos_cal(prev_detections, detections)\n",
    "                        if collision_center:\n",
    "                            collision_frames = 3\n",
    "                # Update previous state\n",
    "                prev_num_objs = curr_num_objs\n",
    "                prev_detections = detections.copy()  # Store current detections for the next frame.\n",
    "\n",
    "                # Draw firework effect\n",
    "                if collision_frames > 0:\n",
    "                    if collision_center:\n",
    "                        frame = collision_effect(collision_center, frame)\n",
    "                    collision_frames -= 1\n",
    "\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 43000,\n",
    "                        \"end\": 55000,\n",
    "                        \"text\": f'''Multi-objects detection. Collision detection with effect. Robust to low-brightness.'''\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            if between(cap, 55000, 58000):\n",
    "                lower_bound = np.array([0, 0, 130])\n",
    "                upper_bound = np.array([32, 165, 255])\n",
    "                mask = bgr_mask(frame, lower_bound, upper_bound)\n",
    "                kernel = np.ones((30, 30), np.uint8)\n",
    "                mask = morphology_operation(mask, cv2.MORPH_DILATE, kernel)\n",
    "                frame = cv2.inpaint(frame, mask, 3, cv2.INPAINT_TELEA)\n",
    "\n",
    "                subtitles = [\n",
    "                    {\n",
    "                        \"start\": 55000,\n",
    "                        \"end\": 58000,\n",
    "                        \"text\": f'''Make tangerines disappear again.'''\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "            frame = add_subtitles_to_frame(subtitles, frame, time)\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Break the loop\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Output\", frame)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 84881,
     "status": "ok",
     "timestamp": 1742157981163,
     "user": {
      "displayName": "Jonathan Pine",
      "userId": "12663940060477254531"
     },
     "user_tz": -60
    },
    "id": "czSdKSyC67L1"
   },
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = 'my_video.mp4'\n",
    "OUTPUT_FILE_NAME = 'my_processed_video.mp4'\n",
    "\n",
    "main(INPUT_FILE_NAME, OUTPUT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
